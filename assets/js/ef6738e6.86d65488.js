"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[8675],{3027:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"Benchmarking/gpu-benchmarking","title":"GPU Benchmarking","description":"CatP2P provides GPU benchmarking functionality to assess the compute capabilities of graphics processing units. This is essential for distributed applications that need to allocate GPU-intensive tasks efficiently.","source":"@site/docs/Benchmarking/gpu-benchmarking.md","sourceDirName":"Benchmarking","slug":"/Benchmarking/gpu-benchmarking","permalink":"/catp2p/docs/Benchmarking/gpu-benchmarking","draft":false,"unlisted":false,"editUrl":"https://github.com/johnnyvillas/catp2p/tree/main/docs-site/docs/Benchmarking/gpu-benchmarking.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Memory Benchmarking","permalink":"/catp2p/docs/Benchmarking/memory-benchmarking"}}');var t=i(4848),a=i(8453);const s={sidebar_position:3},c="GPU Benchmarking",l={},o=[{value:"Overview",id:"overview",level:2},{value:"Basic Usage",id:"basic-usage",level:2},{value:"Simple Benchmark",id:"simple-benchmark",level:3},{value:"Getting GPU Information",id:"getting-gpu-information",level:3},{value:"Using the GPU Benchmark Context",id:"using-the-gpu-benchmark-context",level:3},{value:"Custom Benchmark Configuration",id:"custom-benchmark-configuration",level:3},{value:"Advanced Usage",id:"advanced-usage",level:2},{value:"Analyzing Performance Scaling",id:"analyzing-performance-scaling",level:3},{value:"Visualizing Performance Results",id:"visualizing-performance-results",level:3},{value:"Comparing Matrix Multiplication and Activation Functions",id:"comparing-matrix-multiplication-and-activation-functions",level:3},{value:"Determining Task Suitability",id:"determining-task-suitability",level:3},{value:"Understanding the Benchmarks",id:"understanding-the-benchmarks",level:2},{value:"Matrix Multiplication Benchmark",id:"matrix-multiplication-benchmark",level:3},{value:"Activation Functions Benchmark",id:"activation-functions-benchmark",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Tracking Benchmark Progress",id:"tracking-benchmark-progress",level:2},{value:"API Reference",id:"api-reference",level:2}];function m(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"gpu-benchmarking",children:"GPU Benchmarking"})}),"\n",(0,t.jsx)(e.p,{children:"CatP2P provides GPU benchmarking functionality to assess the compute capabilities of graphics processing units. This is essential for distributed applications that need to allocate GPU-intensive tasks efficiently."}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"The GPU benchmarking module allows you to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Benchmark GPU compute performance using matrix multiplication"}),"\n",(0,t.jsx)(e.li,{children:"Test neural network activation function performance"}),"\n",(0,t.jsx)(e.li,{children:"Get detailed information about the GPU"}),"\n",(0,t.jsx)(e.li,{children:"Compare performance across different graphics cards"}),"\n",(0,t.jsx)(e.li,{children:"Customize benchmark parameters"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,t.jsx)(e.h3,{id:"simple-benchmark",children:"Simple Benchmark"}),"\n",(0,t.jsx)(e.p,{children:"To run a simple GPU benchmark with default settings:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu;\nuse catp2p::error::Error;\n\nfn main() -> Result<(), Error> {\n    // Run a simple benchmark and get the overall score\n    let score = gpu::run_gpu_benchmark()?;\n    println!("GPU benchmark score: {:.2} MFLOPS", score);\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"getting-gpu-information",children:"Getting GPU Information"}),"\n",(0,t.jsx)(e.p,{children:"To get information about the GPU without running a benchmark:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::get_gpu_info;\n\nfn main() -> Result<(), catp2p::error::Error> {\n    // Get GPU information\n    let gpu_info = get_gpu_info()?;\n    \n    println!("GPU: {} ({})", gpu_info.name, gpu_info.vendor);\n    println!("Driver: {}", gpu_info.driver);\n    println!("Estimated VRAM: {}", gpu_info.vram);\n    println!("Backend: {}", gpu_info.backend);\n    println!("Type: {}", if gpu_info.is_integrated { "Integrated" } else { "Discrete" });\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"using-the-gpu-benchmark-context",children:"Using the GPU Benchmark Context"}),"\n",(0,t.jsxs)(e.p,{children:["For more control and better performance, use the ",(0,t.jsx)(e.code,{children:"GpuBenchmarkContext"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::GpuBenchmarkContext;\nuse std::time::Duration;\n\nfn main() -> Result<(), catp2p::error::Error> {\n    // Create a benchmark context\n    let context = GpuBenchmarkContext::new()?;\n    \n    println!("Testing GPU: {}", context.gpu_info.name);\n    \n    // Run matrix multiplication benchmark with different sizes\n    let sizes = [512, 1024, 2048];\n    \n    for &size in &sizes {\n        println!("Running benchmark with {}x{} matrices...", size, size);\n        \n        let result = context.run_matrix_mult(Duration::from_secs(3), size)?;\n        \n        println!("  Score: {:.2} MFLOPS", result.score);\n        println!("  Average FPS: {:.2}", result.average_fps);\n    }\n    \n    // Run activation functions benchmark\n    let data_size = 1_000_000; // 1 million elements\n    println!("Running activation functions benchmark with {} elements...", data_size);\n    \n    let result = context.run_activation_functions(Duration::from_secs(2), data_size)?;\n    \n    println!("  Score: {:.2}", result.score);\n    println!("  Average FPS: {:.2}", result.average_fps);\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"custom-benchmark-configuration",children:"Custom Benchmark Configuration"}),"\n",(0,t.jsx)(e.p,{children:"To run a GPU benchmark with custom configuration:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::{GpuBenchmarkConfig, run_gpu_benchmark_with_config};\n\nfn main() -> Result<(), catp2p::error::Error> {\n    // Create a custom configuration\n    let config = GpuBenchmarkConfig {\n        test_duration_secs: 5,           // Duration of each test in seconds\n        complexity: 8,                   // Higher complexity (larger matrices)\n        ..Default::default()\n    };\n    \n    // Run the benchmark with custom configuration\n    let result = run_gpu_benchmark_with_config(&config)?;\n    \n    println!("Benchmark results for {}:", result.gpu_model);\n    println!("  Compute Score: {:.2} MFLOPS", result.compute_score);\n    println!("  Overall Score: {:.2}", result.overall_score);\n    println!("  Average FPS: {:.2}", result.average_fps);\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-usage",children:"Advanced Usage"}),"\n",(0,t.jsx)(e.h3,{id:"analyzing-performance-scaling",children:"Analyzing Performance Scaling"}),"\n",(0,t.jsx)(e.p,{children:"A common use case is to analyze how GPU performance scales with workload size:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::GpuBenchmarkContext;\nuse std::time::Duration;\nuse std::io::{self, Write};\n\nfn main() -> Result<(), catp2p::error::Error> {\n    // Create a benchmark context\n    let context = GpuBenchmarkContext::new()?;\n    \n    // Run benchmarks with different complexity levels\n    let complexity_levels = [1, 3, 5, 8, 10];\n    let mut results = Vec::new();\n    \n    for &complexity in &complexity_levels {\n        // Calculate matrix size based on complexity\n        let matrix_size = 512 + (complexity * 128);\n        \n        print!("Testing {}x{} matrices... ", matrix_size, matrix_size);\n        io::stdout().flush().unwrap();\n        \n        let result = context.run_matrix_mult(Duration::from_secs(2), matrix_size)?;\n        \n        println!("Score: {:.2} MFLOPS", result.score);\n        results.push((complexity, result.score));\n    }\n    \n    // Find optimal complexity for this GPU\n    if let Some(&(optimal_complexity, max_score)) = results.iter()\n        .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)) {\n        \n        println!("Optimal complexity for this GPU: {}", optimal_complexity);\n        println!("Peak performance: {:.2} MFLOPS", max_score);\n        \n        // Calculate optimal matrix size\n        let optimal_size = 512 + (optimal_complexity * 128);\n        println!("Optimal matrix size: {}x{}", optimal_size, optimal_size);\n    }\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"visualizing-performance-results",children:"Visualizing Performance Results"}),"\n",(0,t.jsx)(e.p,{children:"You can create a simple ASCII chart to visualize performance scaling:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'// After collecting results as (complexity, score) pairs:\nlet max_score = results.iter().map(|&(_, score)| score as usize).max().unwrap_or(1);\nlet scale = 50.0 / max_score as f64;\n\nprintln!("Performance scaling (higher is better):");\nfor &(complexity, score) in &results {\n    let bar_length = (score as f64 * scale) as usize;\n    let bar = "#".repeat(bar_length);\n    println!("Complexity {:2}: {:10.2} MFLOPS |{}|", complexity, score, bar);\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"comparing-matrix-multiplication-and-activation-functions",children:"Comparing Matrix Multiplication and Activation Functions"}),"\n",(0,t.jsx)(e.p,{children:"You can compare different types of GPU workloads to understand your GPU's strengths:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::GpuBenchmarkContext;\nuse std::time::Duration;\n\nfn main() -> Result<(), catp2p::error::Error> {\n    // Create a benchmark context\n    let context = GpuBenchmarkContext::new()?;\n    \n    println!("GPU: {}", context.gpu_info.name);\n    \n    // Run matrix multiplication benchmark\n    let matrix_size = 1024;\n    println!("Running matrix multiplication benchmark ({}x{})...", matrix_size, matrix_size);\n    let matrix_result = context.run_matrix_mult(Duration::from_secs(3), matrix_size)?;\n    println!("Matrix multiplication score: {:.2} MFLOPS", matrix_result.score);\n    \n    // Run activation functions benchmark\n    let data_size = 1_000_000;\n    println!("Running activation functions benchmark ({} elements)...", data_size);\n    let activation_result = context.run_activation_functions(Duration::from_secs(3), data_size)?;\n    println!("Activation functions score: {:.2}", activation_result.score);\n    \n    // Compare the results (note: scores are in different units, this is just for relative performance)\n    if matrix_result.average_fps > activation_result.average_fps {\n        println!("This GPU performs better at matrix operations relative to activation functions");\n    } else {\n        println!("This GPU performs better at activation functions relative to matrix operations");\n    }\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"determining-task-suitability",children:"Determining Task Suitability"}),"\n",(0,t.jsx)(e.p,{children:"You can use the benchmark results to determine if a GPU is suitable for specific tasks:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::GpuBenchmarkContext;\nuse std::time::Duration;\n\nfn main() -> Result<(), catp2p::error::Error> {\n    // Create a benchmark context\n    let context = GpuBenchmarkContext::new()?;\n    \n    // Run a benchmark with a moderate workload\n    let matrix_size = 1024;\n    let result = context.run_matrix_mult(Duration::from_secs(3), matrix_size)?;\n    \n    println!("GPU: {}", context.gpu_info.name);\n    println!("Compute Score: {:.2} MFLOPS", result.score);\n    \n    // Determine suitability for different tasks\n    if result.score > 5_000_000.0 {\n        println!("This GPU is excellent for:");\n        println!("- Complex matrix operations");\n        println!("- Machine learning workloads");\n        println!("- Scientific simulations");\n    } else if result.score > 1_000_000.0 {\n        println!("This GPU is good for:");\n        println!("- Moderate matrix operations");\n        println!("- Basic machine learning tasks");\n        println!("- Data processing");\n    } else if result.score > 100_000.0 {\n        println!("This GPU has limited capabilities for:");\n        println!("- Simple matrix operations");\n        println!("- Basic data processing");\n    } else {\n        println!("This GPU is not recommended for compute tasks");\n        println!("Consider using CPU for these workloads instead");\n    }\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"understanding-the-benchmarks",children:"Understanding the Benchmarks"}),"\n",(0,t.jsx)(e.p,{children:"CatP2P includes two main GPU benchmarks:"}),"\n",(0,t.jsx)(e.h3,{id:"matrix-multiplication-benchmark",children:"Matrix Multiplication Benchmark"}),"\n",(0,t.jsx)(e.p,{children:"The matrix multiplication benchmark is a standard way to measure GPU compute performance:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"What it measures"}),": How quickly the GPU can multiply two large matrices"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Why it matters"}),": Matrix multiplication is a fundamental operation in many GPU workloads:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Machine learning (neural network layers)"}),"\n",(0,t.jsx)(e.li,{children:"Scientific computing"}),"\n",(0,t.jsx)(e.li,{children:"Data processing"}),"\n",(0,t.jsx)(e.li,{children:"Computer graphics (transformations)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"How it works"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Two random matrices of size NxN are created"}),"\n",(0,t.jsx)(e.li,{children:"The matrices are uploaded to GPU memory"}),"\n",(0,t.jsx)(e.li,{children:"A compute shader multiplies the matrices"}),"\n",(0,t.jsx)(e.li,{children:"The process is repeated for the specified duration"}),"\n",(0,t.jsx)(e.li,{children:"Performance is measured in MFLOPS (Millions of Floating Point Operations Per Second)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Matrix size and complexity"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Matrix size is calculated as: 512 + (complexity * 128)"}),"\n",(0,t.jsx)(e.li,{children:"Complexity ranges from 1 to 10"}),"\n",(0,t.jsx)(e.li,{children:"Larger matrices provide more accurate results but may hit memory limits"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"activation-functions-benchmark",children:"Activation Functions Benchmark"}),"\n",(0,t.jsx)(e.p,{children:"The activation functions benchmark measures the GPU's ability to compute common neural network activation functions:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"What it measures"}),": How quickly the GPU can compute activation functions on large arrays of data"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Why it matters"}),": Activation functions are essential operations in neural networks:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Used in every layer of neural networks"}),"\n",(0,t.jsx)(e.li,{children:"Critical for deep learning performance"}),"\n",(0,t.jsx)(e.li,{children:"Representative of many AI workloads"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"How it works"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A large array of random input data is created"}),"\n",(0,t.jsx)(e.li,{children:"The data is uploaded to GPU memory"}),"\n",(0,t.jsxs)(e.li,{children:["A compute shader applies four activation functions to each element:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"ReLU (Rectified Linear Unit)"}),"\n",(0,t.jsx)(e.li,{children:"Sigmoid"}),"\n",(0,t.jsx)(e.li,{children:"Tanh (Hyperbolic Tangent)"}),"\n",(0,t.jsx)(e.li,{children:"Leaky ReLU"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.li,{children:"The process is repeated for the specified duration"}),"\n",(0,t.jsx)(e.li,{children:"Performance is measured in millions of operations per second"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Data size"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Controlled by the ",(0,t.jsx)(e.code,{children:"data_size"})," parameter"]}),"\n",(0,t.jsx)(e.li,{children:"Larger data sizes provide more accurate results but require more memory"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"GPU benchmarking can be resource-intensive and may temporarily impact system performance"}),"\n",(0,t.jsx)(e.li,{children:"For the most accurate results, close other GPU-intensive applications during benchmarking"}),"\n",(0,t.jsx)(e.li,{children:"Results may vary based on driver versions and system conditions"}),"\n",(0,t.jsx)(e.li,{children:"Performance can be affected by thermal throttling during extended tests"}),"\n",(0,t.jsx)(e.li,{children:"Different GPUs have different optimal workload sizes - a GPU might perform better with certain matrix sizes"}),"\n",(0,t.jsx)(e.li,{children:"The benchmark uses compute shaders, which may not be available on very old GPUs"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,t.jsxs)(e.p,{children:["The GPU benchmarking functions return ",(0,t.jsx)(e.code,{children:"Result<T, Error>"})," types, allowing you to handle errors gracefully:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'match GpuBenchmarkContext::new() {\n    Ok(context) => {\n        // Use the context for benchmarking\n    },\n    Err(e) => println!("GPU benchmarking not available: {}", e),\n}\n'})}),"\n",(0,t.jsx)(e.p,{children:"Common errors include:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"No compatible GPU found"}),"\n",(0,t.jsx)(e.li,{children:"Insufficient GPU capabilities for compute shaders"}),"\n",(0,t.jsx)(e.li,{children:"Driver or API compatibility issues"}),"\n",(0,t.jsx)(e.li,{children:"System resource limitations"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"tracking-benchmark-progress",children:"Tracking Benchmark Progress"}),"\n",(0,t.jsx)(e.p,{children:"For long-running benchmarks, you might want to track progress:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-rust",children:'use catp2p::benchmark::gpu::GpuBenchmarkContext;\nuse std::time::{Duration, Instant};\nuse std::io::{self, Write};\n\nfn main() -> Result<(), catp2p::error::Error> {\n    let context = GpuBenchmarkContext::new()?;\n    \n    // Show progress indicators\n    print!("Running matrix multiplication benchmark... ");\n    io::stdout().flush().unwrap();\n    \n    let benchmark_start = Instant::now();\n    let result = context.run_matrix_mult(Duration::from_secs(5), 1024)?;\n    let benchmark_time = benchmark_start.elapsed();\n    \n    println!("Done! (took {:.2?})", benchmark_time);\n    println!("Score: {:.2} MFLOPS", result.score);\n    \n    Ok(())\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,t.jsxs)(e.p,{children:["For detailed API information, see the ",(0,t.jsx)(e.a,{href:"/catp2p/docs/api/benchmark/gpu",children:"GPU Benchmarking API Reference"}),"."]})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>c});var r=i(6540);const t={},a=r.createContext(t);function s(n){const e=r.useContext(a);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);